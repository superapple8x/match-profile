# Development Setup Guide

This guide outlines the steps required to set up and run the Profile Matching application locally for development and testing, including the LLM-driven data analysis feature.

## Prerequisites

*   **Node.js:** Ensure you have Node.js installed (which includes npm). A recent LTS version is recommended (e.g., v18 or later). You can download it from [nodejs.org](https://nodejs.org/).
*   **npm:** Node Package Manager, comes with Node.js.
*   **Git:** For cloning the repository (if not already done).
*   **Docker:** Required for the sandboxed execution of Python code generated by the LLM analysis feature. Download and install Docker Desktop (Windows/Mac) or Docker Engine (Linux) from [docker.com](https://www.docker.com/). Ensure the Docker daemon/service is running.

## Installation

1.  **Clone the Repository (if needed):**
    ```bash
    git clone <repository_url>
    cd match-profile
    ```

2.  **Install Backend Dependencies:** Navigate to the backend directory and install its dependencies.
    ```bash
    cd src/backend
    npm install
    cd ../..
    ```

3.  **Install Frontend Dependencies:** Navigate to the frontend directory and install its dependencies.
    ```bash
    cd src/frontend
    npm install
    cd ../..
    ```

4.  **Configure Docker Permissions (Linux Only):** On Linux, your user needs permission to interact with the Docker daemon. Add your user to the `docker` group:
    ```bash
    sudo usermod -aG docker ${USER}
    ```
    **Important:** You **must log out and log back in** for this change to take effect. Verify by running `docker ps` without `sudo` - it should execute without a permission error.

## Configuration (Backend)

The backend requires environment variables for LLM configuration.

1.  **Create `.env` file:** In the `src/backend` directory, create a file named `.env`.
2.  **Add LLM Settings:** Copy the following content into `src/backend/.env` and **replace the placeholder values with your actual API keys and preferences**:

    ```dotenv
    # LLM Configuration
    LLM_PROVIDER=deepseek # Options: openai, deepseek (gemini, ollama not fully implemented)

    # --- API Keys & Config (Add your keys/prefs here) ---

    # OpenAI (if LLM_PROVIDER=openai)
    OPENAI_API_KEY=your_openai_key
    OPENAI_CODE_MODEL=gpt-3.5-turbo
    OPENAI_TEXT_MODEL=gpt-3.5-turbo

    # DeepSeek (if LLM_PROVIDER=deepseek)
    DEEPSEEK_API_KEY=your_deepseek_key
    DEEPSEEK_BASE_URL=https://api.deepseek.com/v1 # OpenAI compatible endpoint
    DEEPSEEK_CODE_MODEL=deepseek-coder # Or deepseek-chat, deepseek-reasoner
    DEEPSEEK_TEXT_MODEL=deepseek-chat # Or deepseek-reasoner

    # Gemini (if LLM_PROVIDER=gemini) - Requires different SDK/implementation
    # GEMINI_API_KEY=your_gemini_key

    # Ollama (if LLM_PROVIDER=ollama) - Requires local Ollama setup
    # OLLAMA_BASE_URL=http://localhost:11434
    # OLLAMA_CODE_MODEL=codellama
    # OLLAMA_TEXT_MODEL=llama3
    ```
    *   Make sure `LLM_PROVIDER` is set to the service you want to use (e.g., `deepseek`).
    *   Fill in the corresponding API key (`DEEPSEEK_API_KEY` or `OPENAI_API_KEY`).
    *   You can optionally change the model names (e.g., `DEEPSEEK_CODE_MODEL`).

## Running the Application

You need to run both the backend and frontend servers concurrently. Open two separate terminal windows or tabs in the project's root directory (`match-profile`).

1.  **Start the Backend Server:**
    *   In the first terminal, navigate to the backend directory and run the start script:
        ```bash
        cd src/backend
        npm start
        ```
    *   This will typically start the Express server on `http://localhost:3001`. Keep this terminal running. The first time the LLM analysis feature is used, it may take a minute to build the necessary Docker image (`python-analysis-sandbox`).

2.  **Start the Frontend Development Server:**
    *   In the second terminal, navigate to the frontend directory and run the Vite development script:
        ```bash
        cd src/frontend
        npm run dev
        ```
    *   Vite will compile the application and start a development server, usually on `http://localhost:5173`. It will also watch for file changes and automatically update the browser (Hot Module Replacement - HMR). Keep this terminal running.

## Accessing the Application

Once both servers are running:

1.  Open your web browser.
2.  Navigate to the frontend server's address, typically `http://localhost:5173`.

You should now be able to use the Profile Matching application.
*   Upload a CSV file using the "Choose File" and "Upload" buttons.
*   Perform a search using the Search Builder.
*   If results are found, click the "Analyze Results with LLM" button that appears below the results table.
*   Enter a natural language query in the analysis section (e.g., "Plot the distribution of Age") and click "Run Analysis".

The frontend will make requests to the backend server (proxied via Vite) for file import, matching, and LLM analysis.

## Stopping the Servers

To stop the servers, go to each terminal window where they are running and press `Ctrl + C`.